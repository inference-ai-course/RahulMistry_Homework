{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f12e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import faiss\n",
    "import numpy as np\n",
    "import feedparser\n",
    "import pickle\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fastapi import FastAPI\n",
    "from contextlib import asynccontextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "147b6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 1: Data Collection (arXiv API)\n",
    "#############################################\n",
    "\n",
    "def download_arxiv_papers(query=\"cs.CL\", max_results=50, save_dir=\"./papers\"):\n",
    "    \"\"\"\n",
    "    Download PDFs from arXiv based on a query and save them locally.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    params = {\"search_query\": query, \"start\": 0, \"max_results\": max_results}\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to fetch arXiv API\")\n",
    "\n",
    "    feed = feedparser.parse(response.text)\n",
    "    for entry in feed.entries:\n",
    "        pdf_url = None\n",
    "        for link in entry.links:\n",
    "            if link.rel == \"alternate\":\n",
    "                continue\n",
    "            if link.title == \"pdf\":\n",
    "                pdf_url = link.href\n",
    "        if pdf_url:\n",
    "            pdf_name = entry.id.split(\"/\")[-1] + \".pdf\"\n",
    "            pdf_path = os.path.join(save_dir, pdf_name)\n",
    "            if not os.path.exists(pdf_path):\n",
    "                r = requests.get(pdf_url)\n",
    "                with open(pdf_path, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                print(f\"Downloaded {pdf_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12f9bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 2: Text Extraction (PDF â†’ Text)\n",
    "#############################################\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()\n",
    "        pages.append(page_text)\n",
    "    return \"\\n\".join(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b61b17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 3: Chunking Logic\n",
    "#############################################\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "467ecfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 4: Embedding Generation\n",
    "#############################################\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embed_chunks(chunks: List[str]):\n",
    "    return model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0e98479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 5: FAISS Indexing\n",
    "#############################################\n",
    "\n",
    "def build_faiss_index(embeddings: np.ndarray):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a72ffa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 6: Retrieval\n",
    "#############################################\n",
    "\n",
    "def retrieve(query: str, index, chunks: List[str], k=3):\n",
    "    query_vec = model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_vec), k)\n",
    "    results = [chunks[i] for i in indices[0]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd2acf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 7: Retrieval Report\n",
    "#############################################\n",
    "\n",
    "def generate_retrieval_report(queries, index, chunks, report_path=\"retrieval_report.md\", k=3):\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"# Retrieval Report\\n\\n\")\n",
    "        for q in queries:\n",
    "            results = retrieve(q, index, chunks, k)\n",
    "            f.write(f\"## Query: {q}\\n\")\n",
    "            for i, r in enumerate(results, 1):\n",
    "                f.write(f\"**Result {i}:**\\n\\n{r}\\n\\n\")\n",
    "            f.write(\"\\n---\\n\")\n",
    "    print(f\"Retrieval report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "662b828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: format error: No default Layer config\n",
      "\n",
      "Retrieval report saved to retrieval_report.md\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Step 8: FastAPI Service (with lifespan)\n",
    "#############################################\n",
    "\n",
    "chunks = []\n",
    "faiss_index = None\n",
    "\n",
    "@asynccontextmanager\n",
    "def lifespan(app: FastAPI):\n",
    "    global chunks, faiss_index\n",
    "    # Download papers if not already\n",
    "    download_arxiv_papers(query=\"cs.CL\", max_results=50, save_dir=\"./papers\")\n",
    "    all_chunks = []\n",
    "    for fname in os.listdir(\"./papers\"):\n",
    "        if fname.endswith(\".pdf\"):\n",
    "            text = extract_text_from_pdf(os.path.join(\"./papers\", fname))\n",
    "            all_chunks.extend(chunk_text(text))\n",
    "    chunks = all_chunks\n",
    "    if chunks:\n",
    "        embeddings = embed_chunks(chunks)\n",
    "        faiss_index = build_faiss_index(np.array(embeddings))\n",
    "        # save index and chunks\n",
    "        faiss.write_index(faiss_index, \"faiss_index.bin\")\n",
    "        with open(\"chunks.pkl\", \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "        print(\"FAISS index ready with\", len(chunks), \"chunks. Data saved.\")\n",
    "    yield\n",
    "\n",
    "app = FastAPI(lifespan=lifespan)\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(q: str, k: int = 3):\n",
    "    global chunks, faiss_index\n",
    "    if faiss_index is None or not chunks:\n",
    "        return {\"error\": \"Index not built. Please add PDFs first.\"}\n",
    "    query_vector = model.encode([q])\n",
    "    distances, indices = faiss_index.search(np.array(query_vector), k)\n",
    "    results = [chunks[i] for i in indices[0]]\n",
    "    return {\"query\": q, \"results\": results}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Demo run with retrieval report\n",
    "    download_arxiv_papers(query=\"cs.CL\", max_results=10, save_dir=\"./papers\")\n",
    "    all_chunks = []\n",
    "    for fname in os.listdir(\"./papers\"):\n",
    "        if fname.endswith(\".pdf\"):\n",
    "            text = extract_text_from_pdf(os.path.join(\"./papers\", fname))\n",
    "            all_chunks.extend(chunk_text(text))\n",
    "    if all_chunks:\n",
    "        embeddings = embed_chunks(all_chunks)\n",
    "        index = build_faiss_index(np.array(embeddings))\n",
    "        # Save index and chunks\n",
    "        faiss.write_index(index, \"faiss_index.bin\")\n",
    "        with open(\"chunks.pkl\", \"wb\") as f:\n",
    "            pickle.dump(all_chunks, f)\n",
    "\n",
    "        queries = [\n",
    "            \"What are recent advances in machine translation?\",\n",
    "            \"How are language models evaluated?\",\n",
    "            \"What datasets are used in NLP benchmarks?\",\n",
    "            \"Explain attention mechanisms in transformers.\",\n",
    "            \"What is the role of pretraining in NLP models?\"\n",
    "        ]\n",
    "        generate_retrieval_report(queries, index, all_chunks)\n",
    "    else:\n",
    "        print(\"No PDFs found in ./papers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
