# Retrieval Report

## Query: What are recent advances in machine translation?

**Result 1:**

> The ELITR ECA Corpus Philip Williams and Barry Haddow School of Informatics, University of Edinburgh, Scotland {pwillia4,bhaddow}@ed.ac.uk Abstract We present the ELITR ECA corpus, a mul- tilingual corpus derived from publications of the European Court of Auditors. We use au- tomatic translation together with Bleualign to identify parallel sentence pairs in all 506 trans- lation directions. The result is a corpus com- prising 264k document pairs and 41.9M sen- tence pairs. 1 Introduction Most machine translation systems are trained us- ing large volumes of human-translated text. The high cost of human translation means that train- ing data is not typically created speciﬁcally for use in machine translation but is instead repurposed from sources where text has been translated for some other use. The European Union, which pub- lishes in up to 24 languages has long been a rich source of data for the machine translation commu- nity (Koehn, 2005; Steinberger et al., 2006, 2014; Hajlaoui et al., 2014). In line with other institutions of the EU, the Eu- ropean Court of Auditors1 publishes the majority of its ofﬁcial documents in 23 of the 24 EU lan- guages.2 To our knowledge, this data has not yet been compiled into a publicly available corpus. In this work, we downloaded PDF reports from the ECA website and extracted plain text versions to create 23 monolingual corpora, with text seg- mented at the document and sentence level. We then used a multilingual neural machine translation system to automatically translate text in all 506 translation directions and used Bleualign (Sennrich and Volk, 2011) to identify parallel sentences. The resulting dataset comprises 264k document pairs and totalling 41.9M sentence pairs (including du- 1https://www.eca.europa.eu/ 2Irish is due to be included from 2022. plicates) across all translation directions. It is avail- able from http://data.statmt.org/elitr-eca. Sections 2 and 3 describe the methodology in more detail and report some further statistics of the data. 2 Monolingual Corpora We created monolingual corpora in 23 languages by downloading PDF reports, extracting plain text, converting to UTF-8, splitting into sentences, de- hyphenating, and normalizing. The PDFs were downloaded in May 2020. We ﬁrst used the ECA website’s search facility3 to obtain a list of URLs for all available English lan- guage PDFs. ECA documents use a consistent ﬁle naming convention where the ISO 639-1 lan- guage code is included in the ﬁlename (for example agencies 2019 EN.pdf). We substituted the 22 other language codes and downloaded all docu- ments that were available. To obtain plain-text versions of the PDFs, we wrote an Automator script that used macOS’s built- in support for PDF-to-text conversion. The result- ing ﬁles used a mixture of character encodings, so we used chardet4 to automatically detect the encoding of each ﬁle, then converted from that en- coding to UTF-8 using iconv. We split the text into sentences using the split-sentences.perl script from the Moses toolkit (Koehn et al., 2007). A common problem with text extracted from PDF is that words that were broken at the end of a line retain their hyphenation and must be re-joined. We used a simple statistical

**Result 2:**

> Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi´egas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google’s multilingual neural machine translation system: En- abling zero-shot translation. Transactions of the As- sociation for Computational Linguistics, 5:339–351. Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld, Tom Neckermann, Frank Seide, Ulrich Germann, Al- ham Fikri Aji, Nikolay Bogoychev, Andr´e F. T. Mar- tins, and Alexandra Birch. 2018. Marian: Fast neu- ral machine translation in C++. In Proceedings of ACL 2018, System Demonstrations, pages 116–121, Melbourne, Australia. Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit 2005. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the As- sociation for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Ses- sions, pages 177–180, Prague, Czech Republic. Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tok- enizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium. Rico Sennrich and Martin Volk. 2011. Iterative, MT- based sentence alignment of parallel texts. In Pro- ceedings of the 18th Nordic Conference of Compu- tational Linguistics (NODALIDA 2011), pages 175– 182, Riga, Latvia. Northern European Association for Language Technology (NEALT). Ralf Steinberger, Mohamed Ebrahim, Alexandros Poulis, Manuel Carrasco-Benitez, Patrick Schl¨uter, Marek Przybyszewski, and Signe Gilbro. 2014. An overview of the european union’s highly mul- tilingual parallel corpora. Lang. Resour. Eval., 48(4):679–707. Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaˇz Erjavec, Dan Tuﬁs¸, and D´aniel Varga. 2006. The JRC-Acquis: A multilin- gual aligned parallel corpus with 20+ languages. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06), Genoa, Italy. European Language Resources Associ- ation (ELRA). J¨org Tiedemann. 2012. Parallel data, tools and inter- faces in OPUS. In LREC. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998–6008. Curran Asso- ciates, Inc. Biao Zhang, Philip Williams, Ivan Titov, and Rico Sen- nrich. 2020. Improving massively multilingual neu- ral machine translation and zero-shot translation. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 1628– 1639, Online. Association for Computational Lin- guistics.

**Result 3:**

> A Baseline Neural Machine Translation System for Indian Languages Jerin Philip∗ IIIT Hyderabad Vinay P. Namboodiri† IIT Kanpur C. V. Jawahar‡ IIIT Hyderabad Abstract We present a simple, yet effective, Neural Ma- chine Translation system for Indian languages. We demonstrate the feasibility for multiple lan- guage pairs, and establish a strong baseline for further research. 1 Introduction Ability to access information from non-native lan- guages has become a necessity for humans in the modern age of Information Technology. This is specially true in India where there are many pop- ular languages. While knowledge repositories in English are growing at rapid pace, Indian lan- guages still remain very poor in digital resources. This leaves an average Indian handicapped in ac- cessing the knowledge resources. Automatic ma- chine translation is a promising tool to bridge the language barrier, and thereby to democratize the knowledge. Machine Translation (MT) has been a topic of research for over two decades now, with schools of thought following rule-based ap- proaches [Sinha et al., 1995, Chaudhury et al., 2010], example-based approaches [Somers, 1999, Sinhal and Gupta, 2014] and statistics-based ap- proaches [Brown et al., 1993, Koehn, 2009, Kunchukuttan et al., 2014, Banerjee et al., 2018]. A class of statistics-based methods, involving the use of deep neural networks, popularly known as Neural Machine Translation (NMT) is making rapid progress in providing nearly usable transla- tions across many language pairs [Edunov et al., 2018, Aharoni et al., 2019, Neubig and Hu, 2018]. Multiple competitive neural network architectures have come up in the last couple of years, each ∗jerin.philip@research.iiit.ac.in †vinaypn@iitk.ac.in ‡jawahar@iiit.ac.in one outperforming the previous best and estab- lishing superior baselines in a systematic man- ner [Sutskever et al., 2014, Bahdanau et al., 2014, Luong et al., 2015, Gehring et al., 2017, Vaswani et al., 2017]. Unfortunately, most of these meth- ods are highly resource intensive. This, naturally made NMT less attractive for Indian languages due to the concerns of feasibility. However, we be- lieve that the neural machine translation schemes are most appropriate for the Indian languages in the present day setting due to (i) the simplicity of the solution in rapidly prototyping and establishing practically effective systems, (ii) the lower cost of annotation of the training data. (The data required for building NMT systems at most demand sen- tence level alignments, with no special tagging.) (iii) the ease in transfer of ideas/algorithms across languages (Many practical tricks of developing ef- fective solutions in one language provide insights in newer language pairs also.) (iv) ease in transfer of knowledge across languages/tasks, often imple- mented through pre-training, transfer learning or domain adaptation (v) rich software infrastructure available for rapidly prototyping effective mod- els (eg. software stack for training) and deploy- ment (eg. on embedded devices). Many of these are very important in the Indian setting, where the number of researchers and industries interested in Indian Language MT is very small compared to what is demanded. Often we may not even have more than one group equipped per language, with the state of the art tools and techniques. India has 22 official languages,

---

## Query: How are language models evaluated?

**Result 1:**

> arXiv:2303.05834v1 [cs.CL] 10 Mar 2023 AN ALGEBRAIC APPROACH TO TRANSLATING JAPANESE VALENTIN BOBOC Abstract. We use Lambek’s pregroups and the framework of compositional dis- tributional models of language (“DisCoCat”) to study translations from Japanese to English as pairs of functors. Adding decorations to pregroups we show how to handle word order changes between languages. 1. Introduction Language is traditionally viewed as possessing both an empirical aspect – one learns language by practising language – and a compositional aspect – the view that the meaning of a complex phrase is fully determined by its structure and the meanings of its constituent parts. In order to eﬃciently exploit the compositional nature of languages, a popu- lar way of modelling natural languages is a categorical compositional distributional model, abbreviated “DisCoCat” ([CSC10]). Languages are modelled as functors from a category that interprets grammar (“compositional”) to a category that interprets semantics (“distributional”). The compositional part is responsible for evaluating whether phrases or sen- tences are well formed by calculating the overall grammatical type of a phrase from the grammatical types of its individual parts. There are several algebraic methods for modelling the grammar of a natural language. In the present article we choose the well-established model of pregroup grammars. Pregroups were introduced in [Lam97] to replace the algebra of residuated monoids in order to model grammatical types, their juxtapositions, and reductions. Pregroup calculus has been applied to formally represent the syntax of several natural languages such as: French ([BL01a]), Ger- man ([LP04]), Persian ([Sad07]), Arabic ([BL01b]), Japanese ([Car02]), and Latin ([CL05]). The distributional part assigns meanings to individual words by associating to them, for example, statistical co-occurrence vectors ([ML08]). The “DisCoCat” model is thus a way of interpreting compositions of meanings via grammatical structure. In this article we study the notion of translating between compositional dis- tributional models of language by analysing translation from Japanese into English. On the compositional side, a translation is a strong monoidal functor. It is easy to demonstrate that such a functor is too rigid to handle the translation of even sim- ple phrases between languages which have diﬀerent word order. We show that one 1 AN ALGEBRAIC APPROACH TO TRANSLATING JAPANESE 2 can keep using the gadget of monoidal functors as long as the underlying pregroup grammars are decorated with additional structure. We begin by introducing basic notions about the compact closed categories we work with, namely pregroups and ﬁnitely generated vector spaces and deﬁne our no- tion of translation functor. Next, we give an introduction to basic Japanese grammar and the pregroup structure we use to model it. Finally, we introduce notions of pregroup decorations and use them to give a structured framework for translating Japanese sentences. 2. Theoretical background 2.1. Compact closed structures. The key to the “DisCoCat” model is that both the category of pregroups and the category of ﬁnitely generated vector spaces are compact closed categories. This allows for compositional characteristics of grammar to be incorporated into the distributional spaces of meaning. For completeness, we provide here a deﬁnition of compact closure. The reader is encouraged to

**Result 2:**

> of actual languages, and could therefore be employed for purposes beyond the ones for which they were designed. Fours classes of models have been applied. One seeks to approximate the development of linguistic diversity through differential equations and does not operate with languages as having internal structure (Abrams and Strogatz 2003, Nettle 1999c). Another studies the interaction among speakers within a simulated space (a lattice) and also does not operate with any internal language structure (de Oliveira et al. 2006a,b, Patriaraca & Leppännen 2004, Pinasco and Romanelli 2006, Tuncay 2007). A third also studies language dynamics in a simulated space and has some simple way of representing language structure, typically as a string of binary features (bitstrings) (Schulze and Stauffer 2005, Kosmidis et al. 2005, Stauffer et al. 2006, de Oliveira n.d., Teşileanu and Meyer-Ortmanns 2006). Finally, a fourth class of models has elaborate structures for simulating languages, but no component for simulating the interaction among languages and issues of global linguistic diversity. Such models, which are numerous in the field of computational linguistics, fall outside the scope of this review since they are not applied to issues of language dynamics understood as including the interaction among languages. The four classes of model can be summarized in a 2x2 chart as in figure 1. PLEASE INSERT FIGURE 1 AROUND HERE 8 The richest and most versatile type of model will operate with both a space of interaction and an internal structure. A space of interaction may be specified as a geographical space where features such as the effects of geographical distances among languages or physical barriers among them are in the focus of the investigation (Holman et al. 2007, Schulze and Stauffer 2007), or it may be specified more abstractly as a network of interaction such as scale-free networks (Barabási and Albert 1999) or other kinds of networks (e.g. Ke et al., in press), depending on which sort of issue one sets out to investigate. Different sociological models have been applied in different papers, including the different models of Axelrod (1997), Latané (1981), and Nowak et al. (1990). Finally, parameters deriving from basic linguistic knowledge about language dynamics, such as language shift, diffusion, and internal change, have been standard ingredients in much of the work. This section has briefly sketched how language dynamics have been studied over the past few years. In the following section I shall highlight some of the results that I find most interesting as a linguist. 4. Some results 4.1. Stability. It has long been a desideratum to be able to measure how fast different features of language tend to change relative to one another. Some authors who have ventured statements about stabilities of typological features include Nichols (2003) and Croft (1996), and Nichols (1995) suggests different concrete ways of measuring stabilities—what we might call stability metrics. In 9 Wichmann and Holman (n.d.-b) this line of inquiry is broadened to include the entire WALS dataset and different metrics are tested against simulations where there were preset (known) rates of change in languages characterized by a number of features

**Result 3:**

> irrelevant lexical variation within a given syntactic pattern — or a coherent linguistic behavior, such as 90% accuracy on half of the phenomena and only 10% on the other half (consistently selecting the ungrammatical alternative). Overall, both models perform at approximately 50% accuracy; however, only the second model exhibits a sufficient linguistic consistency. I believe that these kinds of ‘technical solutions’ offer effective linking hypotheses between the computational and algorithmic levels, as envisioned by many (Butt this issue; Ramchand this issue), potentially bridging the gap between linguistic theorizing and language modeling in a productive way — likely aligning with the expectations for small-scale experiments proposed by (Onea, Kobayashi, and Wurmbrand this issue). It is important to remember that, to conclude that our theory is explanatorily adequate, we ask more than simple 8 This is not the only possible approach. One might instead adopt a bottom-up perspective, in which LLMs are probed to infer their more-or-less categorical internal representations (Baroni 2022; Zamparelli this issue). 9 I.e., we created a vector of size double than the original word embedding vector, concatenating the first word embedding with the second, then we squeezed this long vector to its original embedding size using a sigmoid transformation. Draft of the paper appearing in the Italian Journal of Linguistics 37.1 2025 DOI: 10.26346/1120-2726-249 implementations of structure-building operations: the model must also be able to bootstrap from Primary Linguistic Data (PLD) and consistently exhibit adult-like performance after reasonable exposure to such input. Without seriously engaging with the acquisitional perspective — in generative linguistic terms, or ‘training’ in computational terms — a theory can, at best, be considered descriptively adequate. On Unboundedness The lack of “grammatical intuitions” in LLMs has also been supported by a separate argument. According to several scholars (Collins 2024; Onea, Kobayashi, and Wurmbrand this issue; Ramchand this issue), LLMs — or more precisely, ANNs — do not qualify as linguistic theories, since they are universal function approximators (Hornik, Stinchcombe, and White 1989), that is, they can simulate Turing-equivalent computations. In my opinion, this is a risky argument: by the same logic, one could also criticize phrase structure grammar (Chomsky 1957), since unconstrained rewriting rules yield Turing-equivalent computational power. Much of Chomsky’s early work focused on identifying the relevant constraints that limit the domain of computation to what is necessary for capturing core linguistic phenomena. There is now a broad consensus that mild context-sensitivity (Shieber 1985) is likely necessary to capture most relevant linguistic properties. If certain scholars are correct, this also applies to ANNs, with the perhaps unsurprising conclusion that Recurrent Neural Networks (RNNs) — due to their inherent recursive mechanisms — are better suited to capturing truly infinite recursion than transformers (Delétang et al. 2022). For years, Chomsky’s hierarchy has served as a foundational tool for researchers introducing new grammatical frameworks, allowing them to demonstrate equivalence with existing formalisms within the mildly context-sensitive domain (Butt this issue; Graf this issue). The rationale was that, since we have efficient algorithms (with polynomial time complexity) for recognition and generation within this domain, we can be confident that our

---

## Query: What datasets are used in NLP benchmarks?

**Result 1:**

> match data of new domains. For NMT on a new domain d, we use θ as the initial param- eters for M and use Dd to further train M to get a domain speciﬁc model M(θd). Note that when ﬁne-tuning onto the target domain, we update all the parameters θ in the model M. Experiments & Analysis We conduct experiments to verify the effectiveness of the proposed model. We follow the same pre- and post- pro- cessing procedures for all the experiments unless otherwise stated. Data We use 7 En-Es parallel datasets of different domains for the evaluation. These datasets are all public available. For fair comparison, only subsets of these datasets with the same amount of sentence pairs are used to simulate NMT with limited data. However, this does not mean that the training data for our proposed model needs to be strictly balanced. The statistics of the datasets used in this work are shown in Table 1. The JRCAcquis (Steinberger et al. 2006) is a legal docu- ment dataset. Global Voices (Prokopidis, Papavassiliou, and Piperidis 2016) is a collection of blogs on various topics. OpenSub is a dataset of Movie and TV subtitles (Lison and Tiedemann 2016). The Europarll (Koehn 2005) and UN Para (Rafalovitch, Dale, and others 2009) come from EU and UN proceedings. Medline (Liu and Cai 2015) is a dataset con- structed from biomedical articles from NIHs MedlinePlus website. EU Bookshops dataset (Skadin¸ˇs et al. 2014) is a collection of publications in EU. All datasets are available on OPUS (Tiedemann 2012) 2, the open parallel corpus, ex- 2http://opus.nlpl.eu/ cept the Medline dataset which is the biomedical domain ESPACMedlineP lus corpus built in (Liu and Cai 2015). Implementations Our proposed model is implemented using Pytorch 3, a ﬂex- ible framework for neural networks. We base our model on the transformer model (Vaswani et al. 2017) and the released Pytorch implementation 4. Parameters are set as follows: word vector size = 300, hidden size = 512, number of lay- ers=4, number of heads=6, dropout=0.3, batch size=64, and beam size=5. The pre-trained English and Spanish embed- ding are obtained using fastText (Mikolov et al. 2018) 5 on the Wikipedia datasets of English and Spanish separately. We use the top 10K En/Es words as base words to construct the base semantic spaces. At testing, we use beam search to ﬁnd the best translated sentences. Decoding ends when every beam gives an < EOS >. Baselines Various methods have been proposed for neural machine translation. Among them, we compare our methods against strong baselines. Transformer We use Transformer as a strong baseline as it has achieved promising performances in several datasets. We use a union of all the training data of different domain datasets for training. Fine Tuning Fine-tuning is a practice used by transfer learning (Zoph et al. 2016). The model is ﬁrstly trained us- ing available data and then ﬁne-tuned using the task-speciﬁc dataset. As mentioned above, ﬁne-tuning aims to ﬁnd local minima for the loss function. It is stable and always achieves results comparable to other state-of-the-art systems (Chu,

**Result 2:**

> for the enormous amount of data points, but so does the consistency with which as many features as possible are attested for the languages included in the database. The online database still has limited accessibility and is moreover entirely in Russian, but it should become open in the near future, and an English version is in preparation. Examples of online databases limited to specific structural features of languages are the UCLA Phonological Segment Inventory Database1, Baerman et al. (2002), or Gast et al. (2007). There are concerns among linguists for developing an infrastructure to facilitate the combination of different databases2, and the first online system for querying several databases simultaneously, The Typological Database System project, has just been launched by a Dutch research group.3 For systematic and computationally supported studies of the lexicon across the world’s languages a comprehensive set of electronic dictionaries organized according to meanings of lexemes is desirable. While dictionaries are available for thousands of languages no such resource exists, however. The Intercontinental Dictionary Series project founded by Mary Ritchie Key and continued by Bernard Comrie4 has the desired standardized electronic format 1 Downloadable as a zipped file from from <http://www.linguistics.ucla.edu/faciliti/sales/upsid.zip>. 2 E.g. E-MELD (http://www.e-meld.org/index.cfm) and GOLD (http://www.linguistics-ontology.org/). 3 See <http://languagelink.let.uu.nl/tds/index.html>. 4 See <http://lingweb.eva.mpg.de/ids/>. 5 and contains up to 1,310 lexical entries per language; the current number of languages represented, however, is only around 250. Another project, the Automated Similarity Judgment Program (ASJP), described in Brown et al. (n.d.),5 has set out to gather short word lists for the purpose of an automated and consistent classification of the world’s languages as well as for statistical investigations of various kinds. Initially 100-item lists, using the ‘Swadesh list’ (e.g. Swadesh 1971) were collected for 245 languages. On the basis of these, the relative stabilities of the items were determined and a reduced list of 40 items selected. At the time of writing, the project members have added many 40-item lists to the original 245 100-item lists, and the total number of languages processed exceeds 1400. The goal is to make the coverage as comprehensive as at all possible. The ASJP lists have made possible large-scale investigations of language dynamics that could not earlier have been undertaken. For studies involving all the world’s languages, a list of these languages, the number of people who speak them as first languages, their locations, and their genealogical classification are necessary, basic pieces of information. The currently best overall catalogue is Gordon et al. (2005), henceforth Ethnologue. A drawback of this catalogue is that it is made for the practical purpose of guiding missionary activities. Thus, it excludes most extinct languages and often is not very rigorous with respect to distinctions between what counts as a dialect and what counts as a language or critical with respect to the genealogical classifictions adopted. Efforts are under way for a more comprehensive catalogue which will remedy the deficiencies of Ethnologue6, but so far Ethnologue is the best single index to the world’s languages. 5 See also <http://email.eva.mpg.de/~wichmann/ASJPHomePage.htm> for updates on the project and links to papers and other

**Result 3:**

> arXiv:1906.11608v2 [cs.CL] 26 Jul 2019 SIMPLE NATURAL LANGUAGE PROCESSING TOOLS FOR DANISH Leon Derczynski Department of Computer Science ITU Copenhagen Denmark DK2300 ld@itu.dk July 29, 2019 ABSTRACT This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available. Keywords natural language processing · Danish · open access 1 Introduction Danish, a language with few resources for automatic processing, is spoken by six million people, largely concentrated in the Scandinavian country of Denmark. At the NLP (natural language processing) group at ITU Copenhagen, one of our foci is Nordic and Danish languages. Thus, we aim to include local languages in our NLP research whenever possible, and to provide tools for others working on these languages. Despite a modest number of researcher in the country, there has been a gap in usable Danish NLP tools that are available to the wider community. As in every time that new technology is not effectively mediated to those who can use it, this restriction stymies the impact and development NLP in Denmark. To bridge this artiﬁcial lacuna, a basic set of easy-to-run utilities that build on existing datasets and open tools has been developed at ITU, which this note introduces. The tools are available at https://nlp.itu.dk/resources/. 2 daner: Named Entity Recognition Named entity recognition means ﬁnding words in text that refer to things by a speciﬁc name. This could be e.g. using “Nanna" to refer to a speciﬁc individual, instead of “them" or “she" or “the person"; or referring to “Moscow" instead of “the nearest city". The important thing is that one mentions a name speciﬁcally. Named entity recognition (NER) ﬁnds those names. In the case of daner, names of Locations, Organisation and Persons are tagged. The daner tool wraps the CoreNLP [1] named entity recognition component [2], using the DKIE data [3] developed as part of the GATE tool [4], which is derived from the UD part of the Copenhagen Dependency Treebank [5], itself including data from the Danish Dependency Treebank [6]. Further ad-hoc data is added as required, from newswire and other sources. The data overall comprises 20K tokens from the CDT (largely 1990s newswire), 3K tokens from non-newswire Danish, and 10K tokens from post-2015 newswire. The mode performs best over Danish newswire text. The data is annotated for three classes, person (PER), location (LOC), and organisation (ORG). Tags are in BIO format. The tool recognises names of people, of organisations, and of locations. It performs automatic tokenization, and outputs in slashed-tag format. For example: En/O stor/O reform/O skal/O derfor/O blandt/O andet/O styrke/O tilliden/O til/O politikere/O og/O medier/O ,/O genopbygge/O tilliden/O til/O Skat/O og/O mindske/O de/O økonomiske/O forskelle/O i/O Danmark/B-LOC ./O Here, “Danmark" is tagged as a location. The B indicates that “Danmark" is the ﬁrst token in the location name (the beginning ). Additional data is added to an internal repository that is also used to train daner; some of this data has

---

## Query: Explain attention mechanisms in transformers.

**Result 1:**

> encoder-decoder architectures for Neural Machine Translation (NMT) [2, 24], and rapidly applied to naturally related tasks such as image captioning (translating an image to a sentence) [25], and summarization (translating to a more compact language) [21]. From a high-level, by allowing the decoder to shop for what it needs over multiple vectors, attention relieves the encoder from the burden of having to embed the input into a single ﬁxed-length vector, and thus allows to keep much more information [1]. Today, attention is ubiquitous in deep learning models, and is not used only in encoder- decoder contexts. Notably, attention devices have been proposed for encoders only, to solve tasks such as document classiﬁcation [27] or representation learning [5]. Such mechanisms are qualiﬁed as self or inner attention. In what follows, we will start by presenting attention in the original context of encoder- decoder for NMT, using the general framework introduced by [20], and then introduce self- attention. 7.1 Encoder-decoder attention 7.1.1 Encoder-decoder overview From a very high level, as shown in Fig. 10, the encoder embeds the input into a vector, and the decoder generates some output from this vector. Figure 10: Overview of the encoder-decoder architecture. Taken from https://sites.google.com/ site/acl16nmt/home In Neural Machine Translation (NMT), the input and the output are sequences of words, re- spectively x =  x1, . . . , xTx  and y =  y1, . . . , yTy  . x and y are usually referred to as the source and target sentences. When both the input and the output are sequences, encoder-decoder ar- chitectures are sometimes called sequence-to-sequence (seq2seq) [24]. Thanks to the fact that encoder-decoder architectures are diﬀerentiable everywhere, their parameters θ can be simulta- neously optimized with maximum likelihood estimation (MLE) over a parallel corpus. This way of training is called end-to-end. argmaxθ ( X (x,y)∈corpus log p(y|x; θ) ) (14) Here, the function that we want to maximize is the log probability of a correct translation. page 14 Notes on Deep Learning for NLP Antoine Tixier, August 2018 7.1.2 Encoder The source sentence can be embedded by any model (e.g., CNN, fully connected). Usually for MT though, the encoder is a deep RNN. Bahdanau et al. [1] originally used a bidirectional deep RNN. Such a model is made of two deep unidirectional RNNs, with diﬀerent parameters except the word embedding matrix. The ﬁrst forward RNN processes the source sentence from left to right, while the second backward RNN processes it from right to left. The two sentence embeddings are concatenated at each time step t to obtain the inner representation of the bidirectional RNN: ht = ⃗ht; ⃗ ht  (15) The bidirectional RNN takes into account the entire context when encoding the source words, not just the preceding words. As a result, ht is biased towards a small window centered on word xt, while with a unidirectional RNN, ht is biased towards xt and the words immediately preceding it. Focusing on a small window around xt may be advantageous, but does not seem crucial. Indeed, Luong et al. [20] obtained state-of-the-art

**Result 2:**

> exp  score(ht, ¯hi)  Ppt+D i′=pt−D exp  score(ht, ¯hi′) exp  −(i −pt)2 2(D/2)2  (22) Note that pt ∈R∩  0, Tx  and i ∈N∩  pt −D, pt +D]. The addition of the Gaussian term makes the alignment weights decay as i moves away from the center of the window pt, i.e., it gives more importance to the annotations near pt. Also, unlike with global attention, the size of αt is ﬁxed and equal to 2D + 1, as only the annotations within the window are taken into account. Local attention can actually be viewed as global attention where alignment weights are multiplied by a truncated Normal distribution (i.e., that returns zero outside the window). A summary of local attention is provided in Fig. 13. page 17 Notes on Deep Learning for NLP Antoine Tixier, August 2018 Figure 13: Summary of the local attention with predictive alignment mechanism [20]. 7.2 Self-attention We are here in a simpler setting with a single RNN encoder taking as input a sequence  x1, . . . , xT  of length T. As usual, the RNN maps the input sequence to a sequence of annotations  h1, . . . , hT  . The goal is exactly the same as with attention in the encoder-decoder context: rather than con- sidering the last annotation hT as a comprehensive summary of the entire sequence, which is prone to information loss, a new hidden representation is computed by taking into account the annotations at all time steps. To this purpose, the self-attention or inner attention mechanism emerged in the literature in 2016/2017, with, e.g., [27, 18]. In what follows we use the formulation of [27]. As shown in Eq. 23, annotation ht is ﬁrst passed to a dense layer. An alignment coeﬃcient αt is then derived by comparing the output ut of the dense layer with a trainable context vector u (initialized randomly) and normalizing with a softmax. The attentional vector s is ﬁnally obtained as a weighted sum of the annotations. ut = tanh(Wht) αt = exp(score(ut, u)) PT t′=1 exp(score(ut′, u)) s = T X t=1 αtht (23) score can in theory be any alignment function. A straightforward approach is to use score(ut, u) = u⊤ t u. The context vector can be interpreted as a representation of the optimal word, on average. When faced with a new example, the model uses this knowledge to decide which word it should pay attention to. During training, through backpropagation, the model updates the context vector, i.e., it adjusts its internal representation of what the optimal word is. 7.2.1 Diﬀerence with seq2seq attention The context vector in the deﬁnition of self-attention above has nothing to do with the context vector used in seq2seq attention! In seq2seq, the context vector ct is equal to the weighted sum PTx i=1 αt,i¯hi, and is used to compute the attentional hidden state as ˜ht = tanh  Wc  ct; ht  . In page 18 Notes on Deep Learning for NLP Antoine Tixier, August 2018 self-attention however, the context vector is

**Result 3:**

> al. 2014) for better handling of long-dependency. A signiﬁcant characteristic of recent NMT models is the wide use of attention mechanism (Bahdanau, Cho, and Bengio 2014; Li, Monroe, and Jurafsky 2016; Vaswani et al. 2017). Attention mechanism is ﬁrstly used in the decoder to enable the decoder to look at the input again and choose the most relevant parts to attend to at each step in translation. Later works like the transformer model (Vaswani et al. 2017) extensively employ the attention mechanism at both the encoder and the decoder sides to capture semantic relations inside sentences. Our proposed model is based on the transformer model, and the detailed description of which can be found in (Vaswani et al. 2017). Domain adaptation for NMT Data sparsity has long been a problem for NMT. Do- main adaptation methods are employed for NMT when the amount of in-domain parallel corpora is insufﬁcient for training a good NMT system. Conventional ways for do- main adaptation are ﬁne-tuning (Luong and Manning 2015; Sennrich, Haddow, and Birch 2016) where models are ﬁrst trained on a high-resource domain or a mixture of data of different domains to initialize parameters which is further trained on the low-resource domain. In-domain ﬁne-tuning comes with at least two shortcomings: ﬁrstly, it depends on the availability of sufﬁcient amounts of in-domain data to avoid over-ﬁtting; secondly, it results in degraded perfor- mance for all other domains. Curriculum learning has also been exploited (Zhang et al. 2019) for domain adaptation. As proved in previous work (Bengio et al. 2009), adjusting the order of training data leads to improvements in both the con- vergence speed and performances. (Wuebker, Simianer, and DeNero 2018) studied the ﬁne tuning process and pointed out that it is possible to do domain adaptation by tuning only a small proportion of the model parameters. This strategy has been adopted by our work by splitting parameters into meta parameters and model parameters. (Zeng et al. 2018) proposed to generate domain-speciﬁc and domain-general representations for words. (Vilar 2018) proposed that dif- ferent neurons play different roles in different domains. It is thus necessary to adjust neurons weights according to data. Instead of manipulating neurons or word representations, we use a neural mapping to consider domain divergence. In this paper we work on the translation of English to Spanish language in different domains. A novel learning pol- icy based on meta learning is proposed to work with the de- signed model. Details are explained in the following section. Meta Learning Meta learning has been drawing much attention of the NLP research community recently due to its ability in learning to transfer knowledge across tasks and domains (Finn, Abbeel, and Levine 2017; Hochreiter, Younger, and Conwell 2001). Meta learning, also known as “learn to learn”, intends to make machine learning models adaptive to a broader cate- gory of tasks/datasets other than the ones they are designed for or trained on. From the perspective of meta learning, training can be regarded as learning a prior over model parameters that is capable for fast adaptation on a new

---

## Query: What is the role of pretraining in NLP models?

**Result 1:**

> same relaxes the computation. A control token is prepended to the input sequence to indicate which direction to trans- late to (__t2xx__). The decoder learns to generate the target given this input. One advantage of using multilingual setups is that it enables zero shot learning. Few shot learn- ing (such as zero-shot and one-shot) has emerged as an effective tool for addressing the lack of training data in many practical problems in com- puter vision [Socher et al., 2013, Zhang and Saligrama, 2015], language processing [Johnson et al., 2017] and speech processing [Stafylakis and Tzimiropoulos, 2018]. Pairs in language direc- tions that did not exist in the training set can still be translated. They can also aid low resource settings of Indian languages. Rapid adaptations to new lan- guages can also be done through multilingual se- tups [Neubig and Hu, 2018]. Aharoni et al. [2019] perform experiments at an even larger scale to find that massive-multilingual setups leading to large improvements in low resource setting. 3.3 Addressing Low Resource In this work, we use a few, rather simple tech- niques to circumvent many challenges and demon- strate the utility for languages that are low- resource. These are briefly discussed. Transfer Learning Zoph et al. [2016] demon- strate the success of transfer learning by using pairs of languages which have rich sentence level aligned bi-text in training low resource languages. A model is trained to take maximum advantage of the rich bi-text in for zz-xx pair, and the model in- stantiated with the weights from zz-xx is used to further train for yy-zz. Backtranslation Using back-translation as a method to augment data used widely in NMT methods today [Sennrich et al., 2015]. In this pro- cess, we train a model which can translate a low resource (LR) language to a high resource (HR) language. This model can attempt augmentation for training data by learning from a pair where it is high resource, followed by mapping the low re- source languages to rich representations in high re- source language. If there is sufficient monolingual data in the source side, this process creates rich parallel pairs. Edunov et al. [2018] take the idea further and tests at scale, to demonstrate improve- ments in translation. 4 Experimental Setup In this section, we detail the datasets and the ex- perimental setup used used in this work. 4.1 Datasets In this work, we compile data available from sev- eral sources, each of which we describe below. The languages we use in our experiments include English (en), Hindi (hi), Bengali (bn), Malayalam (ml), Tamil (ta), Telugu (te) and Urdu (ur). Sum- mary of the corpora used is provided in Table 1. bn hi ml ta te ur wat-train 337K 84K 359K 26K 22K 26K iitb-train - 1.5M - - - - ilci-train 50K 50K 50K 50K 50K 50K wat-dev 0.5K 0.5K 0.5K 0.5K 0.5K 0.5K iitb-dev - 0.5K - - - - iitb-test - 2.5K - - - - wat-test 1K 1K 1K 1K 1K 1K wat-mono 453K 105K 402K 30K 24K 29K wiki- mono 371K - 1M 1.6M

**Result 2:**

> Bordes, A. (2017). Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364. 14 [6] Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14:2, 179-211. 9 [7] Freitag, Markus, and Yaser Al-Onaizan. "Beam search strategies for neural machine transla- tion." arXiv preprint arXiv:1702.01806 (2017). 16 [8] Goldberg, Y. (2015). A primer on neural network models for natural language processing. Journal of Artiﬁcial Intelligence Research, 57, 345-420. 2, 3, 9 [9] Graves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850. 11 [10] Greﬀ, Klaus, et al. "LSTM: A search space odyssey." IEEE transactions on neural networks and learning systems 28.10 (2017): 2222-2232. 14 [11] Hochreiter, S., Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735 11 [12] Hubel, David H., and Torsten N. Wiesel (1962). Receptive ﬁelds, binocular interaction and functional architecture in the cat’s visual cortex. The Journal of physiology 160.1:106-154. 4 [13] Johnson, R., Zhang, T. (2015). Eﬀective Use of Word Order for Text Categorization with Convolutional Neural Networks. To Appear: NAACL-2015, (2011). 2, 8 [14] Kim, Y. (2014). Convolutional Neural Networks for Sentence Classiﬁcation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), 1746–1751. 2, 3, 4 [15] Krizhevsky, Alex, Ilya Sutskever, and Geoﬀrey E. Hinton. "Imagenet classiﬁcation with deep convolutional neural networks." Advances in neural information processing systems. 2012. 4 [16] LeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324. 2, 4 [17] Li, J., Chen, X., Hovy, E., and Jurafsky, D. (2015). Visualizing and understanding neural models in nlp. arXiv preprint arXiv:1506.01066. 8 [18] Lin, Zhouhan, et al. "A structured self-attentive sentence embedding." arXiv preprint arXiv:1703.03130 (2017). 18 [19] Lipton, Zachary C., John Berkowitz, and Charles Elkan. "A critical review of recurrent neural networks for sequence learning." arXiv preprint arXiv:1506.00019 (2015). 9 [20] Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. "Eﬀective approaches to attention-based neural machine translation." arXiv preprint arXiv:1508.04025 (2015). 2, 14, 15, 16, 17, 18 [21] Rush, Alexander M., Sumit Chopra, and Jason Weston. "A neural attention model for abstractive sentence summarization." arXiv preprint arXiv:1509.00685 (2015). 14 [22] Simonyan, K., Vedaldi, A., and Zisserman, A. (2013). Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. "Deep inside convolutional networks: Visualising image classiﬁ- cation models and saliency maps." arXiv preprint arXiv:1312.6034 (2013). arXiv preprint arXiv:1312.6034. 8 page 20 Notes on Deep Learning for NLP Antoine Tixier, August 2018 [23] Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." arXiv preprint arXiv:1412.6806 (2014). 4 [24] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. "Sequence to sequence learning with neural networks." Advances in neural information processing systems. 2014. 2, 6, 14 [25] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., ... & Bengio, Y. (2015, June). Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning (pp. 2048-2057). 14 [26] Zhang, Ye, and Byron Wallace. "A sensitivity analysis of (and practitioners’ guide to) convo- lutional neural networks for

**Result 3:**

> the 21st Century: A Reply to Piantadosi (2023).” arXiv. https://doi.org/10.48550/arXiv.2308.03228. Lan, Nur, Emmanuel Chemla, and Roni Katzir. 2024. “Large Language Models and the Argument from the Poverty of the Stimulus.” Linguistic Inquiry, August, 1–28. https://doi.org/10.1162/ling_a_00533. Lan, Nur, Michal Geyer, Emmanuel Chemla, and Roni Katzir. 2022. “Minimum Description Length Recurrent Neural Networks.” Transactions of the Association for Computational Linguistics 10 (July):785–99. https://doi.org/10.1162/tacl_a_00489. Lillicrap, Timothy P., Adam Santoro, Luke Marris, Colin J. Akerman, and Geoffrey Hinton. 2020. “Backpropagation and the Brain.” Nature Reviews Neuroscience 21 (6): 335–46. https://doi.org/10.1038/s41583-020-0277-3. Marantz, Alec. 2019. “What Do Linguists Do.” Ms., NYU. Draft of the paper appearing in the Italian Journal of Linguistics 37.1 2025 DOI: 10.26346/1120-2726-249 McClelland, James L., and David E. Rumelhart. 1991. Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises. 2. print. Computational Models of Cognition and Perception. Cambridge, Mass.: MIT Pr. Merrill, William, Ashish Sabharwal, and Noah A. Smith. 2022. “Saturated Transformers Are Constant-Depth Threshold Circuits.” Transactions of the Association for Computational Linguistics 10 (August):843–56. https://doi.org/10.1162/tacl_a_00493. Onea, Edgard, Filipe Hisao Kobayashi, and Susy Wurmbrand. this issue. “Reply.” Italian Journal of Linguistics. Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. “BLEU: A Method for Automatic Evaluation of Machine Translation.” In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL ’02, 311. Philadelphia, Pennsylvania: Association for Computational Linguistics. https://doi.org/10.3115/1073083.1073135. Pollard, Carl Jesse, and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. Studies in Contemporary Linguistics. Stanford : Chicago: Center for the Study of Language and Information ; University of Chicago Press. Popper, Karl R. 1934. Logik der Forschung: Zur Erkenntnistheorie der Modernen Naturwissenschaft. Schriften zur Wissenschaftlichen Weltauffassung. Vienna, Austria: Springer Vienna. https://doi.org/10.1007/978-3-7091-4177-9. Quine, Willard Van Orman. 1960. Word and Object. The MIT Press. https://doi.org/10.7551/mitpress/9636.001.0001. Ramchand, Gillian. this issue. “Reply.” Italian Journal of Linguistics. Rizzi, Luigi. this issue. “On the Complementarity of Generative Grammar and Large Language Models.” Italian Journal of Linguistics. Rumelhart, David E., and James L. McClelland. 1986. “On Learning the Past Tenses of English Verbs.” In Parallel Distributed Processing. The MIT Press. https://doi.org/10.7551/mitpress/5237.003.0008. Rumelhart, David E., James L. McClelland, and PDP Research Group. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations. Cambridge (MA): The MIT Press. https://doi.org/10.7551/mitpress/5236.001.0001. Sartran, Laurent, Samuel Barrett, Adhiguna Kuncoro, Miloš Stanojević, Phil Blunsom, and Chris Dyer. 2022. “Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale.” Transactions of the Association for Computational Linguistics 10 (December):1423–39. https://doi.org/10.1162/tacl_a_00526. Draft of the paper appearing in the Italian Journal of Linguistics 37.1 2025 DOI: 10.26346/1120-2726-249 Shieber, Stuart M. 1985. “Evidence against the Context-Freeness of Natural Language.” Linguistics and Philosophy 8 (3): 333–43. https://doi.org/10.1007/BF00630917. Stabler, Edward. 2013. “Two Models of Minimalist, Incremental Syntactic Analysis.” Topics in Cognitive Science 5 (3): 611–33. https://doi.org/10.1111/tops.12031. ———. this issue. “On Linguistic Methodology.” Italian Journal of Linguistics. Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” arXiv:1706.03762 [Cs], December. http://arxiv.org/abs/1706.03762. Warstadt, Alex, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. “BLiMP: The Benchmark

---

