{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 7: Comprehensive Alignment Comparison\n",
    "\n",
    "Welcome to the educational walkthrough for **PPO vs DPO vs GRPO** in LLM alignment!\n",
    "\n",
    "This notebook will guide you step by step through the concepts, code, and experiments for comparing three major alignment methods for language models.\n",
    "\n",
    "**Key Topics:**\n",
    "- Preference data collection\n",
    "- PPO (Proximal Policy Optimization)\n",
    "- DPO (Direct Preference Optimization)\n",
    "- GRPO (Group Relative Policy Optimization)\n",
    "- Evaluation and comparison\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction & Setup\n",
    "\n",
    "This notebook sets up the environment and introduces the main configuration and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Any\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "USE_SMALL_MODELS = True\n",
    "SKIP_OLLAMA = True\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print(\"üéØ Class 7: Alignment Methods Comparison\")\n",
    "print(\"üìä PPO vs DPO vs GRPO Educational Implementation\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Three Methods\n",
    "\n",
    "### PPO (Proximal Policy Optimization)\n",
    "- **Classic RLHF approach** used by ChatGPT\n",
    "- Uses separate reward model + RL training\n",
    "- Formula: `L_PPO = min(r_t(Œ∏)A_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ)A_t)`\n",
    "- **Pros**: Proven, fine control, stable\n",
    "- **Cons**: Complex, 4 models needed, high memory\n",
    "\n",
    "### DPO (Direct Preference Optimization)\n",
    "- **Simpler alternative** that skips reward model\n",
    "- Direct preference optimization\n",
    "- Formula: `L_DPO = -log(œÉ(Œ≤ * preference_diff))`\n",
    "- **Pros**: Simpler, no reward model, stable\n",
    "- **Cons**: Limited to preferences, beta tuning\n",
    "\n",
    "### GRPO (Group Relative Policy Optimization)\n",
    "- **Cutting-edge method** for flexible alignment\n",
    "- Group-based comparisons\n",
    "- Formula: `Advantage = (reward - group_mean) / group_std`\n",
    "- **Pros**: Sample efficient, flexible, latest research\n",
    "- **Cons**: Multiple generations, newer method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Preference Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preference_datasets():\n",
    "    \"\"\"Load comprehensive preference datasets for training\"\"\"\n",
    "    print(\"üìö Loading preference datasets...\")\n",
    "    \n",
    "    # High-quality preference data\n",
    "    preference_data = [\n",
    "        {\n",
    "            \"prompt\": \"How do you debug a memory leak in a Python application?\",\n",
    "            \"chosen\": \"To debug memory leaks in Python: 1) Use memory profilers like memory_profiler or pympler, 2) Identify objects not being garbage collected, 3) Check for circular references, 4) Review global variables and caches, 5) Use weak references appropriately, 6) Monitor memory usage over time, 7) Use tools like objgraph to visualize object references.\",\n",
    "            \"rejected\": \"Just restart the application when it uses too much memory. Memory leaks aren't really a problem in Python.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What's the difference between supervised and unsupervised learning?\",\n",
    "            \"chosen\": \"Supervised learning uses labeled training data where input-output pairs guide the algorithm to learn patterns for prediction. Examples include classification and regression. Unsupervised learning finds hidden patterns in unlabeled data without target outputs, such as clustering and dimensionality reduction. The key difference is the presence of target variables in supervised learning.\",\n",
    "            \"rejected\": \"Supervised learning is when someone supervises the computer while it learns. Unsupervised learning is when the computer learns by itself.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"How do you develop a go-to-market strategy for a new product?\",\n",
    "            \"chosen\": \"A comprehensive GTM strategy includes: 1) Market research and customer segmentation, 2) Value proposition definition, 3) Competitive analysis, 4) Pricing strategy, 5) Distribution channel selection, 6) Marketing and sales strategies, 7) Success metrics and KPIs, 8) Launch timeline and milestones, 9) Risk assessment and contingency plans, 10) Post-launch optimization plan.\",\n",
    "            \"rejected\": \"Just build the product and start selling it. If it's good, people will buy it. Marketing isn't that important.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    datasets_info = {\n",
    "        \"comprehensive_preferences\": {\n",
    "            \"dataset\": Dataset.from_list(preference_data),\n",
    "            \"description\": \"Comprehensive multi-domain preference dataset\",\n",
    "            \"size\": len(preference_data)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Try to load external datasets\n",
    "        hh_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:10]\")\n",
    "        datasets_info[\"hh_rlhf\"] = {\n",
    "            \"dataset\": hh_dataset,\n",
    "            \"description\": \"Anthropic HH-RLHF helpful and harmless\",\n",
    "            \"size\": len(hh_dataset)\n",
    "        }\n",
    "        print(f\"‚úÖ Loaded {len(hh_dataset)} samples from HH-RLHF\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load external datasets: {e}\")\n",
    "    \n",
    "    print(f\"üìä Total datasets available: {len(datasets_info)}\")\n",
    "    return datasets_info\n",
    "\n",
    "# Load datasets\n",
    "datasets_info = load_preference_datasets()\n",
    "for name, info in datasets_info.items():\n",
    "    print(f\"  {name}: {info['description']} (size: {info['size']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Models for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_models():\n",
    "    \"\"\"Setup models for alignment training\"\"\"\n",
    "    print(\"üîß Setting up models for training...\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"üì± Using device: {device}\")\n",
    "    \n",
    "    model_options = [\"distilgpt2\", \"gpt2\"]\n",
    "    \n",
    "    for model_name in model_options:\n",
    "        try:\n",
    "            print(f\"üìÑ Loading {model_name}...\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map=None,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            model = model.to(device)\n",
    "            \n",
    "            # Setup LoRA for efficient training\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                r=8,\n",
    "                lora_alpha=16,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"c_attn\"],\n",
    "                bias=\"none\"\n",
    "            )\n",
    "            \n",
    "            model = get_peft_model(model, lora_config)\n",
    "            model.print_trainable_parameters()\n",
    "            \n",
    "            print(f\"‚úÖ Successfully loaded {model_name}\")\n",
    "            return {\"model\": model, \"tokenizer\": tokenizer, \"name\": model_name, \"device\": device}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    raise RuntimeError(\"Failed to load any model\")\n",
    "\n",
    "# Setup models\n",
    "model_info = setup_models()\n",
    "print(f\"Loaded model: {model_info['name']} on {model_info['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Datasets for Different Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets_for_methods(datasets_info):\n",
    "    \"\"\"Prepare datasets for PPO, DPO, and GRPO training\"\"\"\n",
    "    print(\"üîß Preparing datasets for different alignment methods...\")\n",
    "    \n",
    "    all_preference_data = []\n",
    "    \n",
    "    # Process all datasets\n",
    "    for dataset_name, info in datasets_info.items():\n",
    "        dataset = info[\"dataset\"]\n",
    "        print(f\"  üìä Processing {dataset_name} ({info['size']} samples)...\")\n",
    "        \n",
    "        for example in dataset:\n",
    "            try:\n",
    "                if \"chosen\" in example and \"rejected\" in example:\n",
    "                    preference_example = {\n",
    "                        \"prompt\": str(example.get(\"prompt\", \"\")),\n",
    "                        \"chosen\": str(example[\"chosen\"]),\n",
    "                        \"rejected\": str(example[\"rejected\"])\n",
    "                    }\n",
    "                    \n",
    "                    # Validate data quality\n",
    "                    if all(len(str(preference_example[key]).strip()) > 10 for key in [\"prompt\", \"chosen\", \"rejected\"]):\n",
    "                        all_preference_data.append(preference_example)\n",
    "                        \n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    # Create method-specific datasets\n",
    "    datasets = {}\n",
    "    \n",
    "    if all_preference_data:\n",
    "        # DPO: Direct preference pairs\n",
    "        datasets[\"dpo\"] = Dataset.from_list(all_preference_data)\n",
    "        \n",
    "        # PPO: Queries only (for RL training)\n",
    "        datasets[\"ppo\"] = Dataset.from_list([{\"query\": item[\"prompt\"]} for item in all_preference_data])\n",
    "        \n",
    "        # GRPO: Prompts for group generation\n",
    "        datasets[\"grpo\"] = Dataset.from_list([{\"prompt\": item[\"prompt\"]} for item in all_preference_data])\n",
    "        \n",
    "        print(f\"  ‚úÖ Created datasets - DPO: {len(datasets['dpo'])}, PPO: {len(datasets['ppo'])}, GRPO: {len(datasets['grpo'])}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Prepare datasets\n",
    "alignment_datasets = prepare_datasets_for_methods(datasets_info)\n",
    "for method, dataset in alignment_datasets.items():\n",
    "    print(f\"{method.upper()}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Training Methods\n",
    "\n",
    "### 6.1 PPO Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_ppo_training(model_info, dataset, steps=3):\n",
    "    \"\"\"Simplified PPO training implementation\"\"\"\n",
    "    print(\"üü° Starting PPO Training...\")\n",
    "    print(\"  üéØ Classic RLHF with reward model and RL\")\n",
    "    print(\"  üìä Formula: L_PPO = min(r_t(Œ∏)A_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ)A_t)\")\n",
    "    \n",
    "    if not dataset or len(dataset) == 0:\n",
    "        return {\"status\": \"no_data\", \"message\": \"No dataset available\"}\n",
    "    \n",
    "    model = model_info[\"model\"]\n",
    "    tokenizer = model_info[\"tokenizer\"]\n",
    "    device = model_info[\"device\"]\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for step in range(min(steps, len(dataset))):\n",
    "        query = dataset[step][\"query\"]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            f\"Query: {query}\\nResponse:\",\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        model.train()\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # PPO-style gradient clipping and update\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    # Simulate PPO clipping\n",
    "                    grad_norm = torch.norm(param.grad.data)\n",
    "                    if grad_norm > 1.0:\n",
    "                        param.grad.data = param.grad.data / grad_norm\n",
    "                    \n",
    "                    param.data -= 1e-5 * param.grad.data\n",
    "                    param.grad.zero_()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        print(f\"    üìä PPO Step {step}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"  ‚úÖ PPO training completed!\")\n",
    "    return {\n",
    "        \"status\": \"trained_successfully\",\n",
    "        \"losses\": losses,\n",
    "        \"final_loss\": losses[-1] if losses else 0,\n",
    "        \"steps\": len(losses)\n",
    "    }\n",
    "\n",
    "# Run PPO training\n",
    "ppo_results = simplified_ppo_training(model_info, alignment_datasets[\"ppo\"])\n",
    "print(f\"PPO Results: {ppo_results['status']} - Final Loss: {ppo_results.get('final_loss', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 DPO Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_dpo_training(model_info, dataset, steps=3):\n",
    "    \"\"\"Simplified DPO training implementation\"\"\"\n",
    "    print(\"üü¢ Starting DPO Training...\")\n",
    "    print(\"  üéØ Direct preference optimization without reward model\")\n",
    "    print(\"  üìä Formula: L_DPO = -log(œÉ(Œ≤ * preference_diff))\")\n",
    "    \n",
    "    if not dataset or len(dataset) == 0:\n",
    "        return {\"status\": \"no_data\", \"message\": \"No dataset available\"}\n",
    "    \n",
    "    model = model_info[\"model\"]\n",
    "    tokenizer = model_info[\"tokenizer\"]\n",
    "    device = model_info[\"device\"]\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for step in range(min(steps, len(dataset))):\n",
    "        data = dataset[step]\n",
    "        prompt = data[\"prompt\"]\n",
    "        chosen = data[\"chosen\"]\n",
    "        rejected = data[\"rejected\"]\n",
    "        \n",
    "        # Tokenize chosen and rejected responses\n",
    "        chosen_text = f\"{prompt}\\n{chosen}\"\n
